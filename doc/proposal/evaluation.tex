\section{Evaluation}

Since our tool aims to provide developers with accurate documentation about a program's source code in terms of preconditions, post conditions and usage, the objective of our evaluation is two-fold:\\
\newline
{\bf RQ 1: Does doco generate accurate documentation in terms of pre-conditions, post conditions and usage examples from source code?}

To evaluate this goal, we will carry out an internal evaluation on a well-documented Rust project open-source project called Rust-itertools available on Github.\cite{bluss:2018} It has a well documented collection of usage of function examples along with their associated preconditions. To evaluate doco, we will generate documentation for itertools using doco and compare the generated documentation with the existing one. We will report the accuracy of doco in generating correct conditions for usage, preconditions and post conditions separately.\newline

{\bf RQ 2: Does the doco-generated documentation help developers get up to speed with a new code base and implement functionality quickly and easily?}

We will present a group of 10 graduate students unfamiliar with the Rust-itertools library and ask them to implement 2 pre-defined set of trivial tasks using the functions in the library of similar complexity (require use of same number of Itertools functions). The first set will be implemented by looking at the source-code of the library only while the other will be implemented using the doco-generated documentation. A comparison of the time it took for the developers to implement the two different set of tasks and the usefulness of the doco-generated documentation will be collected using a survey to evaluate the effectiveness of our tools in helping developers.
