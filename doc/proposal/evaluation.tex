\section{Evaluation}

Since our tool aims to provide developers with accurate documentation about a program's source code in terms of pre and post-conditions and usage, the objective of our evaluation is twofold:\\
\newline
{\bf RQ 1: Does \texttt{doco} generate accurate documentation in terms of pre and post-conditions and usage examples from source code?}

To evaluate this goal, we will carry out an internal evaluation on a well-documented open-source Rust project called Itertools available on Github \cite{bluss:2018}. It has a well documented collection of function usage examples along with their associated pre-conditions. To evaluate our methodology, we will generate documentation for itertools using \texttt{doco} and compare it with the existing one. We will report the accuracy of \texttt{doco} in generating correct conditions for usage, preconditions and post conditions separately.\newline

{\bf RQ 2: Does the \texttt{doco}-generated documentation help developers get up to speed with a new code base and implement functionality quickly and easily?}

We will present a group of 10 graduate students unfamiliar with the Rust-itertools library and ask them to implement a set of two pre-defined tasks of similar complexity using the library  (requiring the same number of Itertools functions). The first set of tasks will be implemented by referring to source-code only while the other will be implemented using the \texttt{doco}-generated documentation. A comparison of the time it took for the developers to implement the two different set of tasks and the usefulness of the \texttt{doco}-generated documentation will be collected using a survey to evaluate the effectiveness of our tools in helping developers.
